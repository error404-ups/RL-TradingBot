# ğŸ¤– RL-TradingBot â€” Model-Free DQN Forex Agent

A **model-free Reinforcement Learning** trading bot for **EUR/USD hourly data** using a **Double Dueling DQN** agent. Trained on 10 years of historical data with a custom OpenAI Gymnasium-compatible trading environment.

---

## ğŸ—ï¸ Architecture

```
RL-TradingBot/
â”œâ”€â”€ agents/
â”‚   â””â”€â”€ dqn_agent.py       # Double Dueling DQN + Replay Buffer
â”œâ”€â”€ env/
â”‚   â””â”€â”€ trading_env.py     # Custom Gym environment (ForexTradingEnv)
â”œâ”€â”€ data/
â”‚   â””â”€â”€ data_loader.py     # EUR/USD downloader, cache, train/val/test split
â”œâ”€â”€ models/                # Saved checkpoints
â”œâ”€â”€ logs/                  # Training history JSON
â”œâ”€â”€ results/               # Plots (equity curve, training curves)
â”œâ”€â”€ train.py               # Training script
â”œâ”€â”€ evaluate.py            # Backtest + metrics
â””â”€â”€ requirements.txt
```

---

## ğŸ§  Model-Free RL Algorithm

| Component | Choice |
|-----------|--------|
| Algorithm | **Double DQN** (model-free, off-policy) |
| Network | **Dueling DQN** (separate value + advantage streams) |
| Exploration | **Îµ-Greedy** with exponential decay |
| Memory | **Experience Replay Buffer** (200k transitions) |
| Optimizer | **Adam** with gradient clipping |
| Loss | **Huber (Smooth L1)** |

### Why Double Dueling DQN?
- **Model-free**: no explicit model of market dynamics â€” learns purely from experience
- **Double DQN**: prevents Q-value overestimation by decoupling action selection from evaluation
- **Dueling**: separates "how good is this state?" from "how good is this action?" â€” better for Hold-heavy markets

---

## ğŸ“Š Trading Environment

| Parameter | Value |
|-----------|-------|
| Asset | EUR/USD |
| Timeframe | 1 Hour |
| History | 10 Years (~87,000 bars) |
| Actions | Hold (0), Buy/Long (1), Sell/Short (2) |
| Lookback window | 24 hours |
| Initial balance | $10,000 |
| Spread | 1.5 pips |

### State Features (per candle Ã— 24 bars):
- **Hourly return** (normalized)
- **RSI(14)** (normalized 0â€“1)
- **MACD** (normalized by price)
- **Bollinger Band width**
- **High-Low range**
- **Volume** (z-score normalized)

### Reward Function:
```
reward = realized_PnL (on close) + 0.1 Ã— unrealized_PnL (shaping)
```

---

## ğŸš€ Quick Start

### 1. Install dependencies
```bash
pip install -r requirements.txt
```

### 2. Train the agent
```bash
python train.py
# Or with custom parameters:
python train.py --episodes 300 --lr 3e-4 --batch 256
```

### 3. Evaluate / Backtest
```bash
python evaluate.py --model models/best_model.pt
```

---

## ğŸ“ˆ Data Split

| Set | Ratio | Period (approx) |
|-----|-------|-----------------|
| Train | 70% | 2014â€“2021 |
| Validation | 15% | 2021â€“2022 |
| Test | 15% | 2022â€“2024 |

Data is downloaded automatically via **yfinance** and cached locally as `.parquet` for fast reloads.

---

## âš™ï¸ Hyperparameters

| Parameter | Default |
|-----------|---------|
| Learning rate | 1e-4 |
| Gamma (discount) | 0.99 |
| Îµ start / end / decay | 1.0 / 0.05 / 0.9995 |
| Batch size | 128 |
| Replay buffer | 200,000 |
| Target net update | every 500 steps |
| Hidden units | 256 |
| Episodes | 200 |

---

## ğŸ“‰ Output Artifacts

After training:
- `models/best_model.pt` â€” best checkpoint by validation balance
- `models/checkpoint_epN.pt` â€” periodic checkpoints
- `logs/train_history.json` â€” full reward/loss/epsilon logs
- `results/training_curves.png` â€” 4-panel training dashboard
- `results/backtest.png` â€” price + actions + equity + drawdown

---

## âš ï¸ Disclaimer

This project is for **educational and research purposes only**.  
It does **not** constitute financial advice. Past simulated performance does not guarantee future real-world results. Forex trading involves significant risk.

---

## ğŸ“„ License

MIT License